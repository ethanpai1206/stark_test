#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ•¸æ“šè™•ç†ä¸»æµç¨‹
æ•´åˆJSONæ•¸æ“šç²å–ã€è½‰æ›æˆCSVå’Œåˆä½µè²¡å‹™æ•¸æ“šçš„å®Œæ•´æµç¨‹
"""

import os
import sys
import subprocess
from datetime import datetime

# å°å…¥å„å€‹æ¨¡çµ„
from get_json_data import fetch_json_from_url
import json_to_dataframe
import merge_financial_data

def print_step_header(step_num, step_name):
    """æ‰“å°æ­¥é©Ÿæ¨™é¡Œ"""
    print("\n" + "="*60)
    print(f"æ­¥é©Ÿ {step_num}: {step_name}")
    print("="*60)

def check_file_exists(file_path):
    """æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    return os.path.exists(file_path)

def main():
    """
    ä¸»æµç¨‹å‡½æ•¸
    """
    print("ğŸš€ è‚¡ç¥¨æ•¸æ“šè™•ç†ä¸»æµç¨‹å•Ÿå‹•")
    print(f"åŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # ===== æ­¥é©Ÿ 1: å¾æŒ‡å®šç¶²å€ç²å– JSON æ•¸æ“š =====
        print_step_header(1, "å¾ç¶²å€ç²å– JSON æ•¸æ“š")
        
        # ä½ çš„ Notion URLï¼ˆå¯ä»¥åœ¨é€™è£¡ä¿®æ”¹æˆ–å¾ç”¨æˆ¶è¼¸å…¥ç²å–ï¼‰
        notion_url = "https://file.notion.so/f/f/d70b900c-92f2-4d32-870b-1fa0d80e953b/2cc1982f-a835-4d84-9002-318758475632/output_clean_date_technical.json?table=block&id=f447ef6f-695d-45bb-9e49-f6a9c2e5ddd0&spaceId=d70b900c-92f2-4d32-870b-1fa0d80e953b&expirationTimestamp=1757541600000&signature=tMAguhh67Khr95BrN0qd39SPDMimj3gtXdsbwGQNwmA&downloadName=output_clean_date_technical.json"
        
        # è©¢å•æ˜¯å¦ä½¿ç”¨é è¨­ URL æˆ–è¼¸å…¥æ–°çš„ URL
        use_default = input(f"æ˜¯å¦ä½¿ç”¨é è¨­çš„ Notion URLï¼Ÿ(y/nï¼Œé è¨­ç‚º y): ").strip().lower()
        
        if use_default not in ['y', 'yes', '']:
            notion_url = input("è«‹è¼¸å…¥ JSON æ•¸æ“šçš„ URL: ").strip()
        
        print(f"æ­£åœ¨å¾ URL ç²å– JSON æ•¸æ“š...")
        print(f"URL: {notion_url[:100]}...")  # åªé¡¯ç¤ºå‰100å€‹å­—ç¬¦
        
        # ç²å– JSON æ•¸æ“š
        data = fetch_json_from_url(notion_url)
        
        if not data:
            print("âŒ ç„¡æ³•ç²å– JSON æ•¸æ“šï¼Œç¨‹å¼çµæŸ")
            return
        
        # ä¿å­˜ JSON æ•¸æ“šåˆ°æœ¬åœ°
        json_file_path = '/home/ubuntu/workspace/stark_test/output_data.json'
        with open(json_file_path, 'w', encoding='utf-8') as f:
            import json
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print("âœ… JSON æ•¸æ“šç²å–æˆåŠŸ!")
        print(f"æ•¸æ“šå·²ä¿å­˜åˆ°: {json_file_path}")
        
        # é¡¯ç¤º JSON çµæ§‹
        print("\nğŸ“‹ JSON æ•¸æ“šçµæ§‹:")
        for key in data.keys():
            print(f"- {key}: {type(data[key])}")
            if isinstance(data[key], list):
                print(f"  é•·åº¦: {len(data[key])}")
            elif isinstance(data[key], dict):
                print(f"  éµæ•¸é‡: {len(data[key])}")
        
        # ===== æ­¥é©Ÿ 2: å°‡ JSON è½‰æ›æˆåŸºç¤ CSV æ–‡ä»¶ =====
        print_step_header(2, "å°‡ JSON è½‰æ›æˆåŸºç¤ CSV æ–‡ä»¶")
        
        # ç¢ºä¿ data ç›®éŒ„å­˜åœ¨
        os.makedirs('/home/ubuntu/workspace/stark_test/data', exist_ok=True)
        
        print("æ­£åœ¨è™•ç†æ‰€æœ‰æ•¸æ“šä¸¦è½‰æ›ç‚º CSV...")
        
        # èª¿ç”¨ json_to_dataframe çš„ process_all_data å‡½æ•¸
        all_dataframes = json_to_dataframe.process_all_data(json_file_path)
        
        if not all_dataframes:
            print("âŒ JSON è½‰ CSV å¤±æ•—ï¼Œç¨‹å¼çµæŸ")
            return
        
        # ä¿å­˜æ‰€æœ‰ DataFrame ç‚º CSV æ–‡ä»¶
        saved_files = []
        
        for key, df in all_dataframes.items():
            if df is not None and not df.empty:
                if key == 'historicalPriceFull':
                    file_path = f"data/{key}.csv"
                else:
                    file_path = f"data/{key}.csv"
                
                json_to_dataframe.save_dataframe(df, file_path)
                saved_files.append(file_path)
        
        print("âœ… JSON è½‰ CSV å®Œæˆ!")
        print(f"å·²ç”Ÿæˆ {len(saved_files)} å€‹ CSV æ–‡ä»¶:")
        for file in saved_files:
            print(f"  - {file}")
        
        # æª¢æŸ¥å¿…è¦çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_file = 'data/historicalPriceFull.csv'
        if not check_file_exists(required_file):
            print(f"âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶: {required_file}")
            print("ç„¡æ³•é€²è¡Œä¸‹ä¸€æ­¥åˆä½µï¼Œç¨‹å¼çµæŸ")
            return
        
        # ===== æ­¥é©Ÿ 3: è©¢å•åˆä½µéœ€æ±‚ä¸¦åŸ·è¡Œåˆä½µ =====
        print_step_header(3, "è²¡å‹™æ•¸æ“šåˆä½µ")
        
        print("å³å°‡å•Ÿå‹•è²¡å‹™æ•¸æ“šåˆä½µå·¥å…·...")
        print("æ‚¨å¯ä»¥é¸æ“‡è¦åˆä½µçš„è²¡å‹™æ•¸æ“šè¡¨å’ŒæŠ€è¡“æŒ‡æ¨™è¡¨")
        
        # è©¢å•æ˜¯å¦è¦ç¹¼çºŒé€²è¡Œåˆä½µ
        continue_merge = input("\næ˜¯å¦è¦ç¹¼çºŒé€²è¡Œæ•¸æ“šåˆä½µï¼Ÿ(y/nï¼Œé è¨­ç‚º y): ").strip().lower()
        
        if continue_merge in ['n', 'no']:
            print("âœ… æ•¸æ“šè™•ç†å®Œæˆï¼Œè·³éåˆä½µæ­¥é©Ÿ")
            print(f"æ‚¨å¯ä»¥ç¨å¾Œæ‰‹å‹•åŸ·è¡Œ: python merge_financial_data.py")
            return
        
        print("\nğŸ”„ å•Ÿå‹•æ•¸æ“šåˆä½µå·¥å…·...")
        
        # èª¿ç”¨ merge_financial_data çš„ä¸»å‡½æ•¸
        merge_financial_data.main()
        
        print("\nğŸ‰ ä¸»æµç¨‹åŸ·è¡Œå®Œæˆ!")
        print("æ‰€æœ‰æ­¥é©Ÿå·²æˆåŠŸå®Œæˆ:")
        print("  âœ… JSON æ•¸æ“šç²å–")
        print("  âœ… æ•¸æ“šè½‰æ›ç‚º CSV")
        print("  âœ… è²¡å‹™æ•¸æ“šåˆä½µ")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ¶ä¸­æ­¢ç¨‹å¼åŸ·è¡Œ")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        print("è«‹æª¢æŸ¥éŒ¯èª¤ä¿¡æ¯ä¸¦é‡æ–°åŸ·è¡Œ")
        sys.exit(1)

if __name__ == "__main__":
    main()